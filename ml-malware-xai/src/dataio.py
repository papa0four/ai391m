# src/dataio.py
from __future__ import annotations
import os, glob, warnings
from typing import List, Tuple, Optional, Dict
import numpy as np

# Parquet/CSV will use pandas (import lazily to avoid hard dependency if not needed)
try:
    import pandas as pd
except Exception:  # pragma: no cover
    pd = None


# ----------------------------
# Generic file readers
# ----------------------------
def _load_npz(path: str) -> Tuple[np.ndarray, np.ndarray, List[str], Optional[List[str]]]:
    d = np.load(path, allow_pickle=True)
    X = _as_float32(d["X"])
    y = _as_int64(d["y"])
    feat_names = d["feat_names"].tolist() if "feat_names" in d else [f"f{i}" for i in range(X.shape[1])]
    class_names = d["class_names"].tolist() if "class_names" in d else None
    _validate_xy(X, y)
    return X, y, feat_names, class_names


def _load_sharded_npy(dirpath: str, x_pattern: str = "X_*.npy", y_pattern: str = "y_*.npy"
                      ) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    x_files = sorted(glob.glob(os.path.join(dirpath, x_pattern)))
    y_files = sorted(glob.glob(os.path.join(dirpath, y_pattern)))
    if not x_files or not y_files:
        raise FileNotFoundError(f"No shards found under {dirpath} matching {x_pattern}/{y_pattern}")
    X = np.concatenate([np.load(f, mmap_mode=None) for f in x_files], axis=0).astype(np.float32, copy=False)
    y = np.concatenate([np.load(f, mmap_mode=None) for f in y_files], axis=0).astype(np.int64, copy=False)
    _validate_xy(X, y)
    feat_names = [f"f{i}" for i in range(X.shape[1])]
    return X, y, feat_names


def _load_table(path: str, label_col: str, id_col: Optional[str] = None,
                drop_cols: Optional[List[str]] = None
                ) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    if pd is None:
        raise ImportError("pandas is required to read CSV/Parquet; install pandas/pyarrow.")

    ext = os.path.splitext(path)[1].lower()
    if ext == ".parquet":
        df = pd.read_parquet(path)
    elif ext in (".csv", ".txt"):
        df = pd.read_csv(path)
    else:
        raise ValueError(f"Unsupported table format for {path} (use .parquet or .csv)")

    if label_col not in df.columns:
        raise KeyError(f"label_col '{label_col}' not found in columns: {df.columns.tolist()}")

    if drop_cols:
        for c in drop_cols:
            if c in df.columns:
                df.drop(columns=c, inplace=True)

    y = df.pop(label_col)
    if id_col and id_col in df.columns:
        df.drop(columns=id_col, inplace=True)

    # keep column order as features
    feat_names = df.columns.tolist()
    _assert_all_numeric(df, feat_names)  # raises if non-numeric
    X = df.to_numpy(dtype=np.float32, copy=False)

    # Map y to integers if needed
    if y.dtype.kind in ("O", "U", "S"):  # strings/objects
        classes = sorted(y.unique().tolist())
        mapping = {c: i for i, c in enumerate(classes)}
        y = y.map(mapping).astype(np.int64)
        class_names = classes
    else:
        y = y.to_numpy(dtype=np.int64, copy=False)
        class_names = None  # may set later by caller

    _validate_xy(X, y)
    return X, y, feat_names, class_names


# ----------------------------
# Public helpers / transforms
# ----------------------------
def _validate_xy(X: np.ndarray, y: np.ndarray) -> None:
    if X.ndim != 2:
        raise ValueError(f"X must be 2D [n,d], got shape {X.shape}")
    if y.ndim != 1:
        raise ValueError(f"y must be 1D [n], got shape {y.shape}")
    if X.shape[0] != y.shape[0]:
        raise ValueError(f"Row mismatch: X has {X.shape[0]} rows, y has {y.shape[0]}")


def _as_float32(X: np.ndarray) -> np.ndarray:
    return X.astype(np.float32, copy=False)


def _as_int64(y: np.ndarray) -> np.ndarray:
    return y.astype(np.int64, copy=False)


def _assert_all_numeric(df, feat_names: List[str]) -> None:
    non_numeric = [c for c in feat_names if not np.issubdtype(df[c].dtype, np.number)]
    if non_numeric:
        msg = f"Found non-numeric feature columns: {non_numeric[:8]}{'...' if len(non_numeric)>8 else ''}"
        raise TypeError(msg)


def _limit_rows(X: np.ndarray, y: np.ndarray, limit: Optional[int]) -> Tuple[np.ndarray, np.ndarray]:
    if limit is not None and X.shape[0] > limit:
        return X[:limit], y[:limit]
    return X, y


def select_top_k_classes(y: np.ndarray, k: int, min_count: int = 1
                         ) -> Tuple[np.ndarray, np.ndarray, Dict[int, int], List[int]]:
    """
    Keep only the top-k most frequent classes with at least min_count samples.
    Returns (mask, y_remapped, old_to_new, kept_classes)
    """
    counts = np.bincount(y)
    kept = [c for c in np.argsort(counts)[::-1] if counts[c] >= min_count][:k]
    kept_set = set(kept)
    mask = np.array([cls in kept_set for cls in y], dtype=bool)
    y_kept = y[mask]
    old_to_new = {old: new for new, old in enumerate(kept)}
    y_remap = np.array([old_to_new[old] for old in y_kept], dtype=np.int64)
    return mask, y_remap, old_to_new, kept


def ensure_binary_labels(y: np.ndarray, positive_label: int = 1) -> Tuple[np.ndarray, List[str]]:
    """
    Make sure y is 0/1. If values are not {0,1}, remap smallest to 0 and largest to 1.
    """
    classes = np.unique(y)
    if classes.size != 2:
        raise ValueError(f"Binary task requires exactly 2 classes; got {classes.tolist()}")
    # Map lowest to 0, highest to 1 (or keep if already {0,1})
    mapping = {int(classes[0]): 0, int(classes[1]): 1}
    yb = np.array([mapping[int(v)] for v in y], dtype=np.int64)
    class_names = ["benign", "malicious"] if positive_label == 1 else ["neg", "pos"]
    return yb, class_names


# ----------------------------
# High-level dataset loaders
# ----------------------------
def load_ember2018(data_dir: str,
                   features_file: Optional[str] = None,
                   label_col: str = "label",
                   id_col: Optional[str] = None,
                   drop_cols: Optional[List[str]] = None,
                   limit: Optional[int] = None
                   ) -> Tuple[np.ndarray, np.ndarray, List[str], List[str]]:
    """
    Attempts, in order:
      1) NPZ with keys: X, y, feat_names, (optional) class_names
      2) Parquet/CSV with numeric features + binary label column
      3) Sharded NPY files: X_*.npy / y_*.npy
    Returns: X(float32), y(int64 in {0,1}), feat_names, class_names (['benign','malicious'])
    """
    # 1) NPZ
    if features_file and features_file.endswith(".npz"):
        X, y, feat_names, _ = _load_npz(features_file)
    else:
        npz = features_file if (features_file and os.path.exists(features_file)) else os.path.join(data_dir, "features.npz")
        if os.path.exists(npz):
            X, y, feat_names, _ = _load_npz(npz)
        else:
            # 2) Table (Parquet/CSV)
            table = None
            for candidate in ("features.parquet", "features.csv"):
                p = features_file if (features_file and features_file.endswith(os.path.splitext(candidate)[1])) else os.path.join(data_dir, candidate)
                if os.path.exists(p):
                    table = p; break
            if table:
                X, y, feat_names, _ = _load_table(table, label_col=label_col, id_col=id_col, drop_cols=drop_cols)
            else:
                # 3) Sharded NPY
                X, y, feat_names = _load_sharded_npy(data_dir, "X_*.npy", "y_*.npy")

    # drop unlabeled (-1) rows from EMBER
    if y.dtype.kind in ("i", "u", "f"):  # numeric labels
        mask = (y != -1)
        if mask.shape[0] == y.shape[0] and (~mask).any():
            X, y = X[mask], y[mask]

    # enforce binary labels {0,1}
    y, class_names = ensure_binary_labels(y, positive_label=1)
    X, y = _limit_rows(X, y, limit)
    return X, y, feat_names, class_names


def load_malapi2019(data_dir: str,
                    features_file: Optional[str] = None,
                    label_col: str = "family",
                    id_col: Optional[str] = None,
                    drop_cols: Optional[List[str]] = None,
                    top_k: Optional[int] = None,
                    min_per_class: int = 1,
                    limit: Optional[int] = None
                    ) -> Tuple[np.ndarray, np.ndarray, List[str], List[str]]:
    """
    Multi-class loader with optional top-k class filtering.
    Returns: X(float32), y(int64 in 0..K-1), feat_names, class_names(list[str] length K)
    """
    # 1) NPZ
    if features_file and features_file.endswith(".npz"):
        X, y, feat_names, class_names = _load_npz(features_file)
    else:
        npz = features_file if (features_file and os.path.exists(features_file)) else os.path.join(data_dir, "features.npz")
        if os.path.exists(npz):
            X, y, feat_names, class_names = _load_npz(npz)
        else:
            # 2) Table
            table = None
            for candidate in ("features.parquet", "features.csv"):
                p = features_file if (features_file and features_file.endswith(os.path.splitext(candidate)[1])) else os.path.join(data_dir, candidate)
                if os.path.exists(p):
                    table = p; break
            if table:
                X, y, feat_names, class_names = _load_table(table, label_col=label_col, id_col=id_col, drop_cols=drop_cols)
            else:
                # 3) Sharded NPY
                X, y, feat_names = _load_sharded_npy(data_dir, "X_*.npy", "y_*.npy")
                class_names = [f"class_{i}" for i in range(int(y.max()) + 1)]

    # Optional: keep only top-k classes with minimum count
    if top_k is not None:
        mask, y_remap, old2new, kept = select_top_k_classes(y, k=top_k, min_count=min_per_class)
        X = X[mask]
        y = y_remap
        class_names = [class_names[c] for c in kept]

    X, y = _limit_rows(X, y, limit)
    return X, y, feat_names, class_names

def load_big2015(
    data_dir: str,
    features_file: str | None = None,
    limit: int | None = None,
) -> tuple[np.ndarray, np.ndarray, list[str], list[str]]:
    """
    Load BIG2015 classification data from an NPZ built by big2015_extractor.py.
    Expects X, y, feat_names, class_names in the NPZ.
    """
    npz_path = (
        features_file
        if features_file is not None
        else os.path.join(data_dir, "features.npz")
    )
    X, y, feat_names, class_names = _load_npz(npz_path)
    X, y = _limit_rows(X, y, limit)
    return X, y, feat_names, class_names

def load_bodmas(
    data_dir: str,
    features_file: str | None = None,
    label_col: str = "family",
    id_col: str | None = None,
    drop_cols: list[str] | None = None,
    top_k: int | None = None,
    min_per_class: int = 1,
    limit: int | None = None,
) -> tuple[np.ndarray, np.ndarray, list[str], list[str]]:
    """
    Load BODMAS family classification features.

    - If features_file / features.npz exists: expect X,y,feat_names,class_names.
    - Else, fall back to a features.parquet or features.csv table.
    """
    # 1) Explicit NPZ / features.npz
    if features_file and features_file.endswith(".npz"):
        X, y, feat_names, class_names = _load_npz(features_file)
    else:
        npz_path = (
            features_file
            if (features_file and features_file.endswith(".npz"))
            else os.path.join(data_dir, "features.npz")
        )
        if os.path.exists(npz_path):
            X, y, feat_names, class_names = _load_npz(npz_path)
        else:
            # 2) table case
            table = None
            for candidate in ("features.parquet", "features.csv"):
                p = (
                    features_file
                    if (features_file and features_file.endswith(os.path.splitext(candidate)[1]))
                    else os.path.join(data_dir, candidate)
                )
                if os.path.exists(p):
                    table = p
                    break
            if table is None:
                raise FileNotFoundError(
                    f"No BODMAS features found in {data_dir} "
                    "(expected features.npz / features.parquet / features.csv)."
                )
            X, y, feat_names, class_names = _load_table(
                table,
                label_col=label_col,
                id_col=id_col,
                drop_cols=drop_cols,
            )

    # If NPZ/table didn't provide class_names, synthesize them from y
    if class_names is None:
        # assume y is already integer-encoded 0..C-1
        num_classes = int(y.max()) + 1
        class_names = [f"class_{i}" for i in range(num_classes)]

    # optional top-k filtering / min_per_class
    if top_k is not None:
        mask, y_remap, old2new, kept = select_top_k_classes(
            y, k=top_k, min_count=min_per_class
        )
        X = X[mask]
        y = y_remap
        class_names = [class_names[c] for c in kept]

    X, y = _limit_rows(X, y, limit)
    return X, y, feat_names, class_names