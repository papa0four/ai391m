# src/big2015_extractor.py
#
# Extract simple static features from BIG2015 .bytes files into a single NPZ.
# This version iterates over the .bytes files actually present on disk and
# looks up labels from trainLabels.csv by Id.

import argparse
import math
import os
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd


BIG2015_CLASS_NAME_MAP: Dict[int, str] = {
    1: "Ramnit",
    2: "Lollipop",
    3: "Kelihos_ver3",
    4: "Vundo",
    5: "Simda",
    6: "Tracur",
    7: "Kelihos_ver1",
    8: "Obfuscator.ACY",
    9: "Gatak",
}


def extract_bytes_features(bytes_path: str) -> np.ndarray:
    """
    Extract simple static features from a BIG2015 .bytes file.

    Features:
      - 256-dim normalized byte histogram (0x00-0xFF)
      - log10(file_size_bytes + 1)
      - Shannon entropy (bits)

    Returns:
      features: np.ndarray of shape (258,), dtype float32
    """
    counts = np.zeros(256, dtype=np.int64)
    total_bytes = 0

    with open(bytes_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) <= 1:
                continue

            for tok in parts[1:]:
                if tok == "??":
                    continue
                try:
                    b = int(tok, 16)
                except ValueError:
                    continue
                if 0 <= b <= 255:
                    counts[b] += 1
                    total_bytes += 1

    if total_bytes == 0:
        hist = np.zeros(256, dtype=np.float32)
        entropy = 0.0
        log_size = 0.0
    else:
        hist = counts.astype(np.float32) / float(total_bytes)
        p = hist[hist > 0.0]
        entropy = float(-(p * np.log2(p)).sum())
        log_size = math.log10(float(total_bytes) + 1.0)

    return np.concatenate(
        [hist, np.array([log_size, entropy], dtype=np.float32)]
    ).astype(np.float32)


def build_big2015_npz(
    bytes_dir: str,
    labels_csv: str,
    out_npz: str,
    max_samples: int | None = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Build BIG2015 feature matrix by iterating over .bytes files on disk
    and looking up labels from trainLabels.csv.

    Args:
      bytes_dir: directory containing *.bytes (e.g. data/big2015/bytes)
      labels_csv: path to trainLabels.csv
      out_npz: output .npz path
      max_samples: optional cap; if 0 or None, process all files

    Returns:
      (X, y).
    """
    df = pd.read_csv(labels_csv)
    if "Id" not in df.columns or "Class" not in df.columns:
        raise ValueError("Expected trainLabels.csv to have columns: 'Id' and 'Class'.")

    # Build fast lookup: sample_id -> class (1–9)
    id_to_class: Dict[str, int] = {
        str(row["Id"]): int(row["Class"]) for _, row in df.iterrows()
    }

    if max_samples is not None and max_samples <= 0:
        max_samples = None

    bytes_dir_path = Path(bytes_dir)
    if not bytes_dir_path.exists():
        raise FileNotFoundError(f"bytes_dir does not exist: {bytes_dir}")

    all_bytes_files: List[Path] = sorted(bytes_dir_path.glob("*.bytes"))
    if not all_bytes_files:
        raise RuntimeError(f"No .bytes files found in {bytes_dir}")

    print(f"[big2015] Found {len(all_bytes_files)} .bytes files on disk.")

    X_list: List[np.ndarray] = []
    y_list: List[int] = []

    for idx, path in enumerate(all_bytes_files):
        sample_id = path.stem  # e.g. "02IOCvYEy8mjiuAQHax3"
        if sample_id not in id_to_class:
            # .bytes exists, but no label in trainLabels.csv
            continue

        label = id_to_class[sample_id]
        feats = extract_bytes_features(str(path))

        X_list.append(feats)
        y_list.append(label - 1)  # 1–9 -> 0–8

        if max_samples is not None and len(X_list) >= max_samples:
            break

        if (idx + 1) % 100 == 0:
            print(
                f"[big2015] scanned {idx+1} files, "
                f"collected {len(X_list)} labeled samples..."
            )

    if not X_list:
        raise RuntimeError(
            "No labeled samples were processed; check that your .bytes filenames "
            "match the 'Id' column in trainLabels.csv."
        )

    X = np.stack(X_list, axis=0).astype(np.float32)
    y = np.array(y_list, dtype=np.int64)

    feat_names = [f"byte_hist_0x{b:02X}" for b in range(256)]
    feat_names += ["log_file_size", "entropy"]
    feat_names = np.array(feat_names, dtype="U32")

    class_names = [BIG2015_CLASS_NAME_MAP[i] for i in sorted(BIG2015_CLASS_NAME_MAP)]
    class_names = np.array(class_names, dtype="U32")

    os.makedirs(os.path.dirname(out_npz), exist_ok=True)
    np.savez_compressed(
        out_npz,
        X=X,
        y=y,
        feat_names=feat_names,
        class_names=class_names,
    )

    print(f"[big2015] Saved features to {out_npz}")
    print(f"[big2015] X shape: {X.shape}, y shape: {y.shape}")
    return X, y


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Extract BIG2015 byte-level features into a single NPZ file."
    )
    parser.add_argument(
        "--bytes_dir",
        required=True,
        help="Directory containing BIG2015 .bytes files (e.g. data/big2015/bytes)",
    )
    parser.add_argument(
        "--labels_csv",
        required=True,
        help="Path to trainLabels.csv",
    )
    parser.add_argument(
        "--out_npz",
        required=True,
        help="Output NPZ path (e.g. data/big2015/processed/features.npz)",
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=0,
        help="Optional max samples; 0 means no limit.",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    build_big2015_npz(
        bytes_dir=args.bytes_dir,
        labels_csv=args.labels_csv,
        out_npz=args.out_npz,
        max_samples=args.max_samples,
    )


if __name__ == "__main__":
    main()
