import os, json, joblib, numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    roc_auc_score, average_precision_score, f1_score,
    confusion_matrix, precision_recall_curve, roc_curve
)

# ==== EDIT THESE 2 PATHS IF YOUR TAGS CHANGE ====
NPZ   = r".\outputs\metrics\datasets\ember2018_detect_20251112_010152.npz"
MODEL = r".\outputs\models\ember2018_detect_20251112_010152_catboost_20251112_013241.pkl"
# ===============================================

OUTDIR = r".\outputs\case_study"
TAG = "ember2018_detect_20251111_014225_fulltest"

def load_test_split(npz_path):
    d = np.load(npz_path, allow_pickle=True)
    keys = set(d.keys())
    # try the most common orders
    for Xk, yk in [("X_te","y_te"), ("X_test","y_test"), ("X","y")]:
        if Xk in keys and yk in keys:
            return d[Xk], d[yk], d
    # some pipelines only store train/val/test with "_train/_val/_test"
    if "X_test" in keys and "y_test" in keys:
        return d["X_test"], d["y_test"], d
    raise KeyError(f"Cannot find test split in {npz_path}. Keys: {list(keys)}")

os.makedirs(OUTDIR, exist_ok=True)
CSV_OUT = os.path.join(OUTDIR, f"{TAG}_preds.csv")
MET_JSON = os.path.join(OUTDIR, f"{TAG}_metrics.json")
FIG_CM   = os.path.join(OUTDIR, f"{TAG}_confusion.png")
FIG_ROC  = os.path.join(OUTDIR, f"{TAG}_roc.png")
FIG_PR   = os.path.join(OUTDIR, f"{TAG}_pr.png")

# --- LOAD ---
X, y, d = load_test_split(NPZ)
clf = joblib.load(MODEL)

# --- SCORE ---
scores = clf.predict_proba(X)[:, 1]
pred05 = (scores >= 0.5).astype(int)

# --- METRICS @0.5 ---
auroc = roc_auc_score(y, scores)
auprc = average_precision_score(y, scores)
f1    = f1_score(y, pred05)
tn, fp, fn, tp = confusion_matrix(y, pred05).ravel()

# --- THRESHOLD SWEEP: recall @ target FPRs
fpr, tpr, thr = roc_curve(y, scores)
def recall_at(target):
    m = fpr <= target
    return float(np.max(tpr[m])) if np.any(m) else None

metrics = {
    "dataset": "EMBER 2017+2018 (test)",
    "n_test": int(X.shape[0]),
    "auroc": float(auroc),
    "auprc": float(auprc),
    "f1@0.5": float(f1),
    "confusion@0.5": {"tn": int(tn), "fp": int(fp), "fn": int(fn), "tp": int(tp)},
    "recall@fpr": {
        "0.1%": recall_at(0.001),
        "0.5%": recall_at(0.005),
        "1%":   recall_at(0.01),
    }
}
with open(MET_JSON, "w", encoding="utf-8") as f:
    json.dump(metrics, f, indent=2)

# --- CSV of per-sample predictions
with open(CSV_OUT, "w", encoding="utf-8") as f:
    f.write("index,y_true,score,pred@0.5\n")
    for i,(yt,s,p) in enumerate(zip(y, scores, pred05)):
        f.write(f"{i},{int(yt)},{float(s)},{int(p)}\n")

# --- FIGURES (matplotlib only)
# Confusion
cm = np.array([[tn, fp],[fn, tp]])
plt.figure(figsize=(4.5,4))
plt.imshow(cm, cmap="viridis")
for (i,j),v in np.ndenumerate(cm):
    plt.text(j, i, str(v), ha="center", va="center",
             color="white" if v > cm.max()/2 else "black")
plt.xticks([0,1], ["Benign","Malicious"])
plt.yticks([0,1], ["Benign","Malicious"])
plt.xlabel("Predicted"); plt.ylabel("True"); plt.title("EMBER Confusion @0.5")
plt.tight_layout(); plt.savefig(FIG_CM, dpi=150); plt.close()

# ROC
plt.figure()
plt.plot(fpr, tpr, lw=2); plt.plot([0,1],[0,1],"--",lw=1)
plt.xlabel("FPR"); plt.ylabel("TPR (Recall)")
plt.title(f"ROC (AUROC={auroc:.4f})")
plt.tight_layout(); plt.savefig(FIG_ROC, dpi=150); plt.close()

# PR
prec, rec, _ = precision_recall_curve(y, scores)
plt.figure()
plt.plot(rec, prec, lw=2)
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title(f"PR (AUPRC={auprc:.4f})")
plt.tight_layout(); plt.savefig(FIG_PR, dpi=150); plt.close()

print("Wrote:\n ", MET_JSON, "\n ", CSV_OUT, "\n ", FIG_CM, "\n ", FIG_ROC, "\n ", FIG_PR)
