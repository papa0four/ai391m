from __future__ import annotations
import os, argparse
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy import sparse

ID_CANDIDATES  = ["id", "hash", "sha256", "sample_id", "sha1"]
FAM_CANDIDATES = ["family", "class", "label", "malware_family"]

# ----------------------------
# I/O helpers
# ----------------------------
def read_labels_auto(path: str):
    """
    Returns one of:
      ("mapping", DataFrame[id,family])  OR
      ("list",    Series[family])
    """
    df = None
    try:
        df = pd.read_csv(path, sep=None, engine="python")
    except Exception:
        pass

    if df is None or df.shape[1] == 0:
        for sep in [",",";","\t","|", r"\s+"]:
            try:
                tmp = pd.read_csv(path, sep=sep, engine="python")
                if tmp.shape[1] >= 1:
                    df = tmp
                    break
            except Exception:
                continue

    if df is None or df.shape[1] == 0:
        raise ValueError(f"Could not read labels: {path}")

    if df.shape[1] == 1:
        s = df.iloc[:,0].astype(str).str.strip()
        s = s[s != ""]
        if s.empty:
            raise ValueError("Single-column labels file appears empty after cleaning.")
        return "list", s.reset_index(drop=True)

    cols_lower = [c.lower().strip() for c in df.columns]
    id_col  = next((df.columns[i] for i,c in enumerate(cols_lower) if c in ID_CANDIDATES),  df.columns[0])
    fam_col = next((df.columns[i] for i,c in enumerate(cols_lower) if c in FAM_CANDIDATES), df.columns[1])

    out = df[[id_col, fam_col]].copy()
    out.columns = ["id","family"]
    out["id"] = out["id"].astype(str).str.strip()
    out["family"] = out["family"].astype(str).str.strip()
    out = out[(out["id"]!="") & (out["family"]!="")].dropna()
    if out.empty:
        raise ValueError("Parsed id/family mapping is empty after cleaning.")
    return "mapping", out.reset_index(drop=True)


def stream_docs(text_path: str):
    """
    Yields (sid, "API1 API2 ...") with raw, whitespace-delimited tokens.
    """
    with open(text_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split()
            sid, toks = parts[0], parts[1:]
            if not toks:
                continue
            yield sid, " ".join(toks)

# ----------------------------
# Sanitization
# ----------------------------
def clean_doc(s: str, bad_substrings=("__exception__",)) -> tuple[str, int]:
    """
    Remove any token that contains any of the bad substrings (case-insensitive).
    Returns (cleaned_text, removed_token_count).
    """
    bad_lower = tuple(b.lower() for b in bad_substrings)
    out = []
    removed = 0
    for t in s.split():
        tl = t.lower()
        if any(b in tl for b in bad_lower):
            removed += 1
            continue
        if t:
            out.append(t)
    return " ".join(out), removed

# ----------------------------
# TF-IDF builder
# ----------------------------
def build_tf_idf(labels_csv: str,
                 text_path: str,
                 max_features: int,
                 min_df: int,
                 top_k: int | None,
                 min_per_class: int,
                 limit: int | None):

    mode, lab = read_labels_auto(labels_csv)

    ids, fams, docs = [], [], []

    if mode == "mapping":
        id2fam = dict(zip(lab["id"].astype(str), lab["family"].astype(str)))
        count = 0
        for sid, doc in stream_docs(text_path):
            fam = id2fam.get(str(sid))
            if fam is None:
                continue
            ids.append(str(sid)); fams.append(str(fam)); docs.append(doc)
            count += 1
            if limit is not None and count >= limit:
                break
    else:
        fam_list = lab.astype(str).tolist()
        count = 0
        for sid, doc in stream_docs(text_path):
            if count >= len(fam_list):
                break
            ids.append(str(sid)); fams.append(fam_list[count]); docs.append(doc)
            count += 1
            if limit is not None and count >= limit:
                break

    if not ids:
        raise RuntimeError("No matched documents. Check labels vs all_analysis_data.txt quantities/format.")

    # ---------- pre-vectorization sanitization ----------
    total_removed = 0
    cleaned_docs = []
    for d in docs:
        cd, r = clean_doc(d, bad_substrings=("__exception__",))
        total_removed += r
        cleaned_docs.append(cd)
    # Drop rows that became empty after cleaning
    keep_mask = [bool(c.strip()) for c in cleaned_docs]
    if not any(keep_mask):
        raise RuntimeError("All documents became empty after sanitization.")
    docs = [c for c, k in zip(cleaned_docs, keep_mask) if k]
    fams = [f for f, k in zip(fams, keep_mask) if k]
    ids  = [i for i, k in zip(ids,  keep_mask) if k]

    if total_removed > 0:
        print(f"[INFO] Sanitization removed {total_removed} tokens containing '__exception__' "
              f"across {sum(1 for k in keep_mask if k)} kept documents.")

    # Vectorize AFTER cleaning
    vect = TfidfVectorizer(
        token_pattern=r"[^ \t\r\n]+",  # whitespace split
        max_features=max_features,
        min_df=min_df
    )
    Xs = vect.fit_transform(docs)
    feat_names = np.array(vect.get_feature_names_out(), dtype=object)

    # ---------- post-fit safety net: drop any residual '__exception__' feature ----------
    bad_feat_mask = np.array([("__exception__" in str(n).lower()) for n in feat_names])
    if bad_feat_mask.any():
        keep_idx = np.where(~bad_feat_mask)[0]
        Xs = Xs[:, keep_idx]
        feat_names = feat_names[keep_idx]
        print(f"[WARN] Dropped {bad_feat_mask.sum()} feature(s) containing '__exception__' after vectorization.")

    # Optional family filtering
    vc = pd.Series(fams).value_counts()
    keep_fams = set(vc[vc >= min_per_class].head(top_k).index.tolist()) if top_k is not None else set(vc.index.tolist())
    mask = [f in keep_fams for f in fams]
    if not any(mask):
        raise RuntimeError("Family filter removed all rows. Try lowering --min_per_class or drop --top_k.")
    Xs = Xs[mask]
    fams = [f for f, m in zip(fams, mask) if m]

    class_names = sorted(set(fams))
    fam2idx = {c: i for i, c in enumerate(class_names)}
    y = np.array([fam2idx[f] for f in fams], dtype=np.int64)

    # Densify if small; else keep CSR
    if Xs.shape[0] * Xs.shape[1] <= 50_000_000:
        X = Xs.astype(np.float32).toarray()
        return X, y, feat_names.tolist(), class_names
    else:
        X = sparse.csr_matrix(Xs, dtype=np.float32)
        return X, y, feat_names.tolist(), class_names

# ----------------------------
# CLI
# ----------------------------
def main():
    ap = argparse.ArgumentParser(
        description="Build Mal-API-2019 TF-IDF features.npz (mapping or list labels supported)"
    )
    ap.add_argument("--labels_csv", required=True)
    ap.add_argument("--text_path", required=True)
    ap.add_argument("--out_npz", required=True)
    ap.add_argument("--max_features", type=int, default=10000)
    ap.add_argument("--min_df", type=int, default=5)
    ap.add_argument("--top_k", type=int, default=None)
    ap.add_argument("--min_per_class", type=int, default=50)
    ap.add_argument("--limit", type=int, default=None)
    args = ap.parse_args()

    X, y, feat_names, class_names = build_tf_idf(
        args.labels_csv, args.text_path, args.max_features, args.min_df,
        args.top_k, args.min_per_class, args.limit
    )

    os.makedirs(os.path.dirname(args.out_npz), exist_ok=True)

    # Save dense or CSR
    if sparse.issparse(X):
        np.savez_compressed(
            args.out_npz,
            data=X.data, indices=X.indices, indptr=X.indptr, shape=X.shape,
            y=y,
            feat_names=np.array(feat_names, dtype=object),
            class_names=np.array(class_names, dtype=object)
        )
    else:
        np.savez_compressed(
            args.out_npz,
            X=np.asarray(X, dtype=np.float32),
            y=y,
            feat_names=np.array(feat_names, dtype=object),
            class_names=np.array(class_names, dtype=object)
        )
    print(f"[OK] Wrote {args.out_npz} with X shape {X.shape}, classes={len(class_names)}")

if __name__ == "__main__":
    main()
